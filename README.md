# Mallama ğŸ¦™

**Mallama** is a lightweight, self-hosted web UI for running local LLMs with **Ollama**.  
Itâ€™s designed to be simple, fast, and hackable. No cloud. No accounts. Your models, your machine.

---

## âœ¨ Features

- ğŸ–¥ï¸ Clean web-based chat interface
- ğŸ”Œ Uses local Ollama models (no external APIs)
- âš¡ Fast startup, minimal dependencies
- ğŸ§  Model selection directly from the UI
- ğŸ§ª Designed for experimentation and extension
- ğŸ§ Linux-friendly, self-hosted by default

![Mallama UI](./images/screenshot1.png)
![Chat View](./images/screenshot2.png)
![Settings](./images/screenshot3.png)

---

## ğŸš€ Quick Start

### Requirements
- Python 3.9+
- Ollama installed and running

### Run locally

```bash
git clone https://github.com/mesut2ooo/mallama.git
cd mallama
```
or
```python
pip install -r requirements.txt
python app.py
```

Then open your browser at:
http://127.0.0.1:5000

Thatâ€™s it.

---
ğŸ¯ Why Mallama?
Most local LLM UIs are either:
Overly complex
Heavy on resources
Hard to modify
Mallama aims to be:
Easy to understand
Easy to modify
Easy to run on modest hardware
Itâ€™s built for developers, tinkerers, and self-hosting enthusiasts.

---
ğŸ› Feedback & Contributions
This project is actively looking for feedback.
Found a bug? Open an issue.
Have an idea? Start a discussion.
Want to contribute? PRs are welcome.
If something feels awkward, slow, or confusing â€” thatâ€™s valuable feedback.
